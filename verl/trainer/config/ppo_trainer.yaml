data:
  tokenizer: null
  train_files: null
  val_files: null
  prompt_key: prompt
  max_prompt_length: 512
  max_response_length: 512
  train_batch_size: 1024
  val_batch_size: 1024
  return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs
  return_raw_chat: False
  shuffle: True
  max_pixels: 4194304
  min_pixels: 262144

actor_rollout_ref:
  hybrid_engine: True
  model:
    path: null
    override_config: {}
    enable_gradient_checkpointing: True
    use_remove_padding: False
  actor:
    
  ref:
    fsdp_config:
      param_offload: False


algorithm:
  gamma: 1.0
  lam: 1.0
  adv_estimator: gae
  kl_penalty: kl  # how to estimate kl divergence
  kl_ctrl:
    type: fixed
    kl_coef: 0.001

trainer:
  total_epochs: 30
  total_training_steps: null
  project_name: verl_examples
  experiment_name: gsm8k
  logger: ['console', 'wandb']
  val_generations_to_log_to_wandb: 0
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: -1
  # auto: find the last ckpt to resume. If can't find, start from scratch
  resume_mode: disable # or auto or resume_path if
  resume_from_path: False
  val_before_train: true
  test_freq: -1
  critic_warmup: 0
  default_hdfs_dir: null
  remove_previous_ckpt_in_save: false
  del_local_ckpt_after_load: false
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
