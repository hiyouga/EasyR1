# Number Game RL Training Architecture

## ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OFFLINE RL TRAINING SYSTEM                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1       â”‚
â”‚  æ•°æ®æ”¶é›†       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Android    â”‚      â”‚  Android    â”‚      â”‚  Android    â”‚
    â”‚  Device 1   â”‚      â”‚  Device 2   â”‚      â”‚  Device 3   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚                    â”‚                    â”‚
           â”‚    ADB over USB/WiFi                   â”‚
           â”‚                    â”‚                    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ collect_game â”‚
                         â”‚  _data.py    â”‚
                         â”‚              â”‚
                         â”‚ ThreadPool   â”‚
                         â”‚ Executor     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Raw Screenshots      â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ question.png     â”‚  â”‚
                    â”‚  â”‚ result.png       â”‚  â”‚
                    â”‚  â”‚ metadata.json    â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2       â”‚
â”‚  æ•°æ®æ ‡æ³¨       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         annotate_game_data.py              â”‚
    â”‚                                            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚  PaddleOCR   â”‚    â”‚   Ollama VLM    â”‚  â”‚
    â”‚  â”‚              â”‚    â”‚  (qwen2.5-vl)   â”‚  â”‚
    â”‚  â”‚ Extract:     â”‚    â”‚                 â”‚  â”‚
    â”‚  â”‚ - 3 numbers  â”‚    â”‚  Extract:       â”‚  â”‚
    â”‚  â”‚ - positions  â”‚    â”‚  - light color  â”‚  â”‚
    â”‚  â”‚              â”‚    â”‚  - correct ans  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚         â”‚                     â”‚            â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
    â”‚                    â”‚                       â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
    â”‚         â”‚  Quality Filtering  â”‚            â”‚
    â”‚         â”‚  (VLM confidence)   â”‚            â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  EasyR1 JSONL Format â”‚
              â”‚                      â”‚
              â”‚  train.jsonl         â”‚
              â”‚  test.jsonl          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3       â”‚
â”‚  RLè®­ç»ƒ        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              EasyR1 Training Framework              â”‚
    â”‚                                                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚         Data Loading & Preprocessing        â”‚    â”‚
    â”‚  â”‚  â€¢ Load JSONL                              â”‚    â”‚
    â”‚  â”‚  â€¢ Apply format_prompt (game_rules.jinja)  â”‚    â”‚
    â”‚  â”‚  â€¢ Tokenize & encode images               â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                     â”‚                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚         Actor Worker (Training)            â”‚    â”‚
    â”‚  â”‚  â€¢ Model: Qwen2-VL-2B-Instruct            â”‚    â”‚
    â”‚  â”‚  â€¢ Optimizer: AdamW (lr=5e-6)             â”‚    â”‚
    â”‚  â”‚  â€¢ FSDP: Full Sharding                    â”‚    â”‚
    â”‚  â”‚  â€¢ Gradient Checkpointing                 â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                     â”‚                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚       Rollout Worker (Generation)          â”‚    â”‚
    â”‚  â”‚  â€¢ Generate n=8 responses per prompt      â”‚    â”‚
    â”‚  â”‚  â€¢ Temperature: 1.0                       â”‚    â”‚
    â”‚  â”‚  â€¢ vLLM backend for fast inference        â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                     â”‚                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚      Reward Worker (Scoring)               â”‚    â”‚
    â”‚  â”‚  â€¢ compute_score(response, ground_truth)  â”‚    â”‚
    â”‚  â”‚  â€¢ Correct = 1.0, Wrong = 0.0             â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                     â”‚                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚      Reference Worker (KL baseline)        â”‚    â”‚
    â”‚  â”‚  â€¢ Original model (frozen)                â”‚    â”‚
    â”‚  â”‚  â€¢ Compute KL divergence penalty          â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                     â”‚                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚           GRPO Algorithm                   â”‚    â”‚
    â”‚  â”‚                                            â”‚    â”‚
    â”‚  â”‚  1. Compute group advantages:             â”‚    â”‚
    â”‚  â”‚     Adv_i = R_i - mean(R_group)           â”‚    â”‚
    â”‚  â”‚                                            â”‚    â”‚
    â”‚  â”‚  2. Policy gradient:                      â”‚    â”‚
    â”‚  â”‚     âˆ‡L = E[Adv * âˆ‡log Ï€(a|s)]            â”‚    â”‚
    â”‚  â”‚                                            â”‚    â”‚
    â”‚  â”‚  3. KL penalty:                           â”‚    â”‚
    â”‚  â”‚     L_KL = Î² * KL(Ï€ || Ï€_ref)            â”‚    â”‚
    â”‚  â”‚                                            â”‚    â”‚
    â”‚  â”‚  4. Update policy:                        â”‚    â”‚
    â”‚  â”‚     Î¸ â† Î¸ - lr * (âˆ‡L + âˆ‡L_KL)            â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Trained VLM Model    â”‚
                â”‚                        â”‚
                â”‚  Checkpoints saved     â”‚
                â”‚  every 5 epochs        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## æ•°æ®æµè¯¦è§£

### 1. æ•°æ®æ”¶é›†æµç¨‹

```
User Input:
  --device-id device1 device2
  --num-episodes 100

                    â†“

For each device in parallel:
  â”‚
  â”œâ”€ Connect to device via ADB
  â”‚
  â”œâ”€ For each episode (1 to 100):
  â”‚   â”‚
  â”‚   â”œâ”€ Calculate episode_id (auto-increment)
  â”‚   â”‚
  â”‚   â”œâ”€ For each round (1 to 5):
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ Capture question screenshot
  â”‚   â”‚   â”‚   â†’ episode_X_round_Y_question.png
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ Random select option (0/1/2)
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ Click on selected card
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ Capture result screenshot
  â”‚   â”‚   â”‚   â†’ episode_X_round_Y_result.png
  â”‚   â”‚   â”‚
  â”‚   â”‚   â””â”€ Click NEXT button
  â”‚   â”‚
  â”‚   â””â”€ Save metadata.json
  â”‚       {
  â”‚         "episode_id": X,
  â”‚         "rounds": [
  â”‚           {
  â”‚             "round": Y,
  â”‚             "question_screenshot": "...",
  â”‚             "result_screenshot": "..."
  â”‚           }
  â”‚         ]
  â”‚       }
  â”‚
  â””â”€ Done

Output:
  game_data/
    â”œâ”€ device1/
    â”‚   â”œâ”€ episode_1_metadata.json
    â”‚   â”œâ”€ episode_1_round_1_question.png
    â”‚   â”œâ”€ episode_1_round_1_result.png
    â”‚   â””â”€ ...
    â””â”€ device2/
        â””â”€ ...
```

### 2. æ•°æ®æ ‡æ³¨æµç¨‹

```
Input: Raw screenshots from all devices

For each episode in all devices:
  â”‚
  For each round:
    â”‚
    â”œâ”€ Load question.png
    â”‚   â”‚
    â”‚   â”œâ”€ PaddleOCR.predict()
    â”‚   â”‚   â”‚
    â”‚   â”‚   â”œâ”€ Extract all text + positions
    â”‚   â”‚   â”‚
    â”‚   â”‚   â”œâ”€ Filter: confidence > 0.5
    â”‚   â”‚   â”‚
    â”‚   â”‚   â”œâ”€ Filter: y > 600 (card area)
    â”‚   â”‚   â”‚
    â”‚   â”‚   â”œâ”€ Select top 3 by confidence
    â”‚   â”‚   â”‚
    â”‚   â”‚   â””â”€ Sort by x-coordinate (leftâ†’right)
    â”‚   â”‚       â†’ numbers = [5, 8, 3]
    â”‚   â”‚
    â”‚   â””â”€ VLM: extract light color
    â”‚       â†’ light_color = "GREEN"
    â”‚
    â”œâ”€ Load result.png
    â”‚   â”‚
    â”‚   â””â”€ VLM: extract correct answer index
    â”‚       â†’ correct_index = 1
    â”‚
    â”œâ”€ Generate problem text:
    â”‚   "<image>Light: GREEN. Numbers: 5, 8, 3. Select the correct one."
    â”‚
    â”œâ”€ Quality filtering:
    â”‚   â”‚
    â”‚   â”œâ”€ Check VLM response cleanliness
    â”‚   â”‚   â€¢ Is answer in ["0", "1", "2"]? âœ“
    â”‚   â”‚   â€¢ Is response concise? âœ“
    â”‚   â”‚   â€¢ Response length < 20 chars? âœ“
    â”‚   â”‚
    â”‚   â””â”€ Decision: HIGH_CONFIDENCE â†’ keep
    â”‚
    â””â”€ Create training sample:
        {
          "images": ["path/to/question.png"],
          "problem": "<image>Light: GREEN. Numbers: 5, 8, 3...",
          "answer": "1"
        }

Shuffle all samples

Split:
  â€¢ 80% â†’ train.jsonl
  â€¢ 20% â†’ test.jsonl

Save stats:
  â€¢ total_samples
  â€¢ low_confidence_filtered
  â€¢ failed_rounds
```

### 3. GRPOè®­ç»ƒæµç¨‹

```
Epoch Loop (1 to 20):
  â”‚
  For each batch in train_data:
    â”‚
    â”œâ”€ 1. ROLLOUT PHASE
    â”‚   â”‚
    â”‚   â”œâ”€ Load images + apply format_prompt
    â”‚   â”‚
    â”‚   â”œâ”€ Generate n=8 responses per prompt (vLLM)
    â”‚   â”‚   Input: "<image>Light: GREEN. Numbers: 5, 8, 3..."
    â”‚   â”‚   Output: ["1", "0", "1", "1", "2", "1", "0", "1"]
    â”‚   â”‚
    â”‚   â””â”€ Compute rewards (parallel)
    â”‚       ground_truth = "1"
    â”‚       rewards = [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]
    â”‚
    â”œâ”€ 2. ADVANTAGE ESTIMATION (GRPO)
    â”‚   â”‚
    â”‚   â”œâ”€ Group mean reward:
    â”‚   â”‚   R_mean = (1.0+0.0+1.0+1.0+0.0+1.0+0.0+1.0) / 8 = 0.625
    â”‚   â”‚
    â”‚   â””â”€ Compute advantages:
    â”‚       Adv = [0.375, -0.625, 0.375, 0.375, -0.625, 0.375, -0.625, 0.375]
    â”‚
    â”œâ”€ 3. POLICY GRADIENT
    â”‚   â”‚
    â”‚   â”œâ”€ Compute log probabilities:
    â”‚   â”‚   log_Ï€(response | prompt) for each of 8 responses
    â”‚   â”‚
    â”‚   â”œâ”€ Policy loss:
    â”‚   â”‚   L_pg = -mean(Adv * log_Ï€)
    â”‚   â”‚
    â”‚   â””â”€ KL divergence penalty:
    â”‚       KL = KL(Ï€_current || Ï€_ref)
    â”‚       L_kl = Î² * KL  (Î² = 0.01)
    â”‚
    â”œâ”€ 4. BACKWARD & UPDATE
    â”‚   â”‚
    â”‚   â”œâ”€ Total loss: L = L_pg + L_kl
    â”‚   â”‚
    â”‚   â”œâ”€ Backward propagation (FSDP)
    â”‚   â”‚
    â”‚   â”œâ”€ Gradient clipping (max_norm=1.0)
    â”‚   â”‚
    â”‚   â””â”€ Optimizer step (AdamW, lr=5e-6)
    â”‚
    â””â”€ Log metrics:
        â€¢ mean_reward
        â€¢ mean_advantage
        â€¢ kl_divergence
        â€¢ policy_loss

  Validation (every 2 epochs):
    â”‚
    â”œâ”€ Generate predictions on test set
    â”‚
    â”œâ”€ Compute accuracy
    â”‚
    â””â”€ Log sample predictions to WandB

  Save checkpoint (every 5 epochs)

End training
```

## å…³é”®ç»„ä»¶è¯´æ˜

### Actor Worker (è®­ç»ƒæ¨¡å‹)
- **ä½œç”¨**: è¢«è®­ç»ƒçš„ç­–ç•¥ç½‘ç»œ
- **è¾“å…¥**: (image, prompt) â†’ VLM model
- **è¾“å‡º**: æ–‡æœ¬å“åº” "0"/"1"/"2"
- **ä¼˜åŒ–ç›®æ ‡**: æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±
- **æŠ€æœ¯**: FSDPå…¨åˆ†ç‰‡ï¼Œæ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜

### Rollout Worker (ç”Ÿæˆå“åº”)
- **ä½œç”¨**: å¿«é€Ÿç”Ÿæˆå¤šä¸ªå“åº”æ ·æœ¬
- **åç«¯**: vLLMï¼ˆPagedAttentionåŠ é€Ÿï¼‰
- **é‡‡æ ·ç­–ç•¥**: temperature=1.0 ç¡®ä¿æ¢ç´¢
- **æ‰¹å¤„ç†**: å¹¶è¡Œç”Ÿæˆå¤šä¸ªpromptçš„å“åº”

### Reward Worker (è®¡ç®—å¥–åŠ±)
- **ä½œç”¨**: è¯„ä¼°æ¯ä¸ªå“åº”çš„è´¨é‡
- **é€»è¾‘**: 
  ```python
  if extract_answer(response) == ground_truth:
      reward = 1.0
  else:
      reward = 0.0
  ```
- **å¹¶è¡ŒåŒ–**: æ‰¹é‡è®¡ç®—æé«˜æ•ˆç‡

### Reference Worker (KLåŸºå‡†)
- **ä½œç”¨**: ä¿æŒç­–ç•¥ä¸åç¦»åŸæ¨¡å‹å¤ªè¿œ
- **æ¨¡å‹**: åŸå§‹Qwen2-VL-2Bï¼ˆå†»ç»“å‚æ•°ï¼‰
- **è®¡ç®—**: KL(Ï€_current || Ï€_ref)
- **æƒ©ç½šç³»æ•°**: Î² = 0.01

## å†…å­˜ä¼˜åŒ–ç­–ç•¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Memory Optimization Hierarchy       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Level 1: Model Sharding (FSDP)
  â€¢ å°†æ¨¡å‹å‚æ•°åˆ‡åˆ†åˆ°å¤šå¼ GPU
  â€¢ enable_full_shard: true
  â†“ Saves ~60% GPU memory

Level 2: Gradient Checkpointing
  â€¢ ä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼
  â€¢ åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—
  â†“ Saves ~40% GPU memory (slower training)

Level 3: CPU Offloading
  â€¢ offload_params: true
  â€¢ offload_optimizer: true
  â†“ Saves GPU memory, uses more CPU RAM

Level 4: Dynamic Batching
  â€¢ æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´batch size
  â€¢ padding_free: true
  â†“ Reduces padding overhead

With all optimizations:
  Qwen2-VL-2B can run on 2x 16GB GPUs
  Qwen2-VL-7B can run on 2x 24GB GPUs
```

## è®­ç»ƒç›‘æ§æŒ‡æ ‡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Key Metrics to Monitor           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Reward Metrics:
   â€¢ mean_reward:      0.33 â†’ 0.90+ (good convergence)
   â€¢ reward_std:       Track variance in group rewards
   â€¢ max_reward:       Should approach 1.0

2. Policy Metrics:
   â€¢ policy_loss:      Should decrease over time
   â€¢ kl_divergence:    Keep < 0.1 (not drifting too far)
   â€¢ entropy:          Track exploration level

3. Validation Metrics:
   â€¢ val_accuracy:     0.33 â†’ 0.90+ (target)
   â€¢ val_accuracy_by_light:
     - GREEN accuracy
     - RED accuracy
     - YELLOW accuracy

4. Training Efficiency:
   â€¢ samples_per_sec:  Throughput
   â€¢ gpu_utilization:  Should be >80%
   â€¢ time_per_epoch:   Track training speed

Expected Training Curve:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 1.0 â”¤                        â•­â”€ â”‚ val_accuracy
  â”‚     â”‚                    â•­â”€â”€â”€â•¯   â”‚
  â”‚ 0.8 â”¤               â•­â”€â”€â”€â”€â•¯       â”‚
  â”‚     â”‚          â•­â”€â”€â”€â”€â•¯            â”‚
  â”‚ 0.6 â”¤      â•­â”€â”€â”€â•¯                 â”‚
  â”‚     â”‚  â•­â”€â”€â”€â•¯                     â”‚
  â”‚ 0.4 â”¤â”€â”€â•¯                         â”‚
  â”‚     â”‚                            â”‚
  â”‚ 0.2 â”¤                            â”‚
  â”‚     â”‚                            â”‚
  â”‚ 0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚     0    5   10   15   20        â”‚
  â”‚          Epoch                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## æŠ€æœ¯æ ˆæ€»ç»“

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | åŸå›  |
|------|---------|------|
| RLç®—æ³• | GRPO | ç¦»çº¿æ•°æ®ï¼Œgroup-basedä¼˜åŠ¿ä¼°è®¡ |
| VLMæ¨¡å‹ | Qwen2-VL-2B | å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦ |
| æ¨ç†åç«¯ | vLLM | PagedAttentioné«˜åå |
| è®­ç»ƒæ¡†æ¶ | PyTorch + FSDP | å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ |
| ä»»åŠ¡è°ƒåº¦ | Ray | å¤šworkerååŒ |
| OCRå¼•æ“ | PaddleOCR | é«˜å‡†ç¡®ç‡ä¸­æ–‡/æ•°å­—è¯†åˆ« |
| VLMæœåŠ¡ | Ollama | æœ¬åœ°éƒ¨ç½²æ–¹ä¾¿ |

---

**ç¥è®­ç»ƒæˆåŠŸï¼ğŸš€**
